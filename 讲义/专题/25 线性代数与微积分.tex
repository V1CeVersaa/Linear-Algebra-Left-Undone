\chapter{线性代数与微积分}

\section{线性映射的分析性质}

% 参考：《矩阵分析及其应用》张贤达
%      《矩阵分析》Horn
%      《泛函分析讲义》许全华
%      《数学分析讲义》于品

我们都熟知下面一点：$\mathcal{L}(\mathbf{R}^n, \mathbf{R}^m)$ 与 $\mathbf{R}^{m\times n}$ 同构，所以在某种意义上，我们研究线性映射与研究矩阵其实是一个效果. 线性映射其实只是线性空间之间的特别的函数，之前我们研究的东西一般多集中于线性映射的代数性质上，对于连续、有界等等更分析一些的性质我们并没有深入讨论. 本节我们将讨论线性映射的一些分析性质. 首先我们将从线性映射的范数切入：

\begin{definition}{线性映射的范数}{}
    对于线性映射 $A\in \mathcal{L}(\mathbf{R}^n, \mathbf{R}^m)$ 与 $x\in\mathbf{R}^n$，定义 $A$ 的\term{范数} $\Vert A \Vert$ 为 $\Vert A \Vert = \sup\limits_{\Vert x\Vert =  1} \Vert Ax\Vert$.
    % $\Vert A \Vert = \sup\limits_{\Vert x\Vert\leqslant 1} \Vert Ax\Vert$
\end{definition}

有些书籍称这样定义的范数为\term{诱导范数}，大体是因为这种形式的定义不区分使用线性映射还是矩阵. 诱导范数反映出了线性映射伸张向量的能力，根据\[\Vert A\Vert = \sup\limits_{x\neq 0}\Vert A\frac{x}{\Vert x\Vert}\Vert = \sup\limits_{x\neq 0}\frac{\Vert Ax\Vert}{\Vert x\Vert},\]以及\[\Vert Ax\Vert = \Vert x\Vert \Vert A\frac{x}{\Vert x\Vert}\Vert \leqslant \Vert x\Vert \Vert A\Vert,\]这样就可以定义线性映射 $A$ 在 $x$ 方向的\term{伸张系数}$\dfrac{\Vert Ax\Vert}{\Vert x\Vert}$，并且 $\Vert A\Vert$ 的几何意义是一切方向的伸张系数的上确界.

注意到，对于一般的线性空间 $X$，如果范数定义得当，就算是 $X$ 是无穷维的，我们对有限维线性空间上的线性映射定义的范数施用在无穷维线性空间上的线性映射也是有意义的，但我们敏锐的嗅觉告诉我们，一旦加上了“无穷维”这个不容易处理的条件，无穷维线性空间上的线性映射的性质就会变得更加复杂，我们暂且不对其继续深入讨论.

诱导范数确实满足下面四条性质，读者可以简单验证：

\begin{theorem}{}{}
    设 $A\in \mathcal{L}(\mathbf{R}^n, \mathbf{R}^m),\enspace B\in \mathcal{L}(\mathbf{R}^l, \mathbf{R}^n)$，则
    \begin{enumerate}
        \item $\Vert A\Vert\geqslant 0$，且 $\Vert A\Vert=0$ 当且仅当 $A=0$；
        \item 若$\lambda\in \mathbf{R}$，$\Vert \lambda A\Vert = \vert\lambda\vert \Vert A\Vert$；
        \item $\Vert A+B\Vert\leqslant\Vert A\Vert+\Vert B\Vert$；
        \item $\Vert AB\Vert\leqslant\Vert A\Vert\Vert B\Vert$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item 显然；
        \item $\Vert \lambda A\Vert=\sup\limits_{\Vert x\Vert =  1}\Vert \lambda Ax\Vert = \vert\lambda\vert \sup\limits_{\Vert x\Vert = 1}\Vert Ax\Vert = \vert\lambda\vert \Vert A\Vert$；
        \item $\Vert A+B\Vert=\sup\limits_{\Vert x\Vert = 1}\Vert (A+B)x\Vert = \sup\limits_{\Vert x\Vert = 1}\Vert Ax + Bx\Vert\leqslant\sup\limits_{\Vert x\Vert = 1}\Vert Ax\Vert+\sup\limits_{\Vert x\Vert = 1}\Vert Bx\Vert=\Vert A\Vert+\Vert B\Vert$；
        \item $\Vert AB\Vert=\sup\limits_{\Vert x\Vert = 1}\Vert ABx\Vert=\sup\limits_{\Vert x\Vert = 1}\Vert A(Bx)\Vert\leqslant\sup\limits_{\Vert x\Vert = 1}\Vert A\Vert\Vert Bx\Vert\leqslant\Vert A\Vert\sup\limits_{\Vert x\Vert = 1}\Vert Bx\Vert=\Vert A\Vert\Vert B\Vert$.
    \end{enumerate}
\end{proof}

按我们诱导范数的定义，很容易证明任一矩阵的范数肯定大于其任何一个特征值的绝对值. 对于任何形式定义的范数，这个陈述也一样成立，证明并不困难，我们将其留作习题.

% 参考：https://math.stackexchange.com/questions/2855044/why-is-the-norm-of-a-matrix-larger-than-its-eigenvalue

将向量的内积加以推广，我们还可以定义矩阵的内积与另一种范数：

\begin{definition}{矩阵的内积}{}
    对于 $\mathbf{F}^{m\times n}$ 上的矩阵 $A$ 与 $B$，$A = \left[\vec{a_1}, \vec{a_2}, \dots, \vec{a_n}\right]$ 与 $B = \left[\vec{b_1}, \vec{b_2}, \dots, \vec{b_n}\right]$，将其写成列向量拼接的形式：定义矩阵 $A$ 的\textrm{列向量化}为$mn \times 1$ 向量 \[\vec{a} = \mathrm{vec}(A) = \begin{pmatrix} \vec{a_1} \\ \vec{a_2} \\ \vdots \\ \vec{a_n} \end{pmatrix},\]
    借此定义矩阵 $A$ 与 $B$ 的内积为 $\langle A, B\rangle \colon\ \mathbf{F}^{m\times n}\times\mathbf{F}^{m\times n}\to\mathbf{F}$，\[\langle A, B\rangle = \langle \mathrm{vec}(A), \mathrm{vec}(B)\rangle.\]
    常见形式为 Frobenius 内积 \[\langle A, B\rangle = \mathrm{tr}(A^{T}B) = \sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}b_{ij}.\]
\end{definition}

这样定义的内积高度依赖于我们向量内积的选取，并且仅对于矩阵有意义. 借助这种矩阵向量化的思想，我们可以定义矩阵的\textrm{“元素形式”范数}：

\begin{definition}{“元素形式”范数}{}
    对于矩阵 $M\in \mathbf{F}^{m\times n}$ 定义 $M$ 的\textrm{“元素形式”范数}为下面的 $p$ 矩阵范数 \[\Vert M\Vert_p = \Vert \mathrm{vec}(M) \Vert_p = \left(\sum_{i = 1}^{m}\sum_{j = 1}^{n} \vert a_{ij}\vert^p\right)^{1/p}.\]
    很容易可以验证其满足范数的四条性质.
\end{definition}

这样一来，我们成功将线性映射组成的线性空间成功“升格”成了赋范线性空间，之后甚至可以在其之上继续研究，这就形成了泛函分析中丰富的算子理论，但与本节相隔甚远，仅介绍一些较为基础的内容. 恰如我们在实数域等赋范空间上定义了多种范数一样，在线性映射组成的线性空间中，我们可以根据需求的不同使用不同的范数进行研究，但是重要的是知道不同范数之间可能存在的关系. 幸运的是，在有限维赋范线性空间上，不同范数之间是等价的，让我们一步一步来看：

赋范线性空间 $(X, \Vert \cdot \Vert)$ 当然是度量空间，对于 $x, y\in X$，只需要定义 $d(x, y) = \Vert x-y\Vert$ 就可以轻易诱导出一个度量，而连续与收敛的概念恰恰就定义在度量空间上，不妨回忆一下先：

\begin{definition}{收敛}{}
    设 $V$ 是一个实的或者复的赋范空间，其范数为 $\Vert \cdot \Vert$，此范数诱导出的度量为 $d(x, y) = \Vert x - y\Vert$，称 $V$ 中的一个序列 $\{x^{(i)}\}$ \term{收敛}到 $x\in V$，当且仅当 \[\lim_{i\to\infty}d(x^{(i)} - x) = 0.\]
\end{definition}

\begin{definition}{连续}{}
    假设 $(X,d_X)$ 和 $(Y, d_Y)$ 是两个距离空间，$f: X\to Y$ 是这两个距离空间的映射. 假设 $x_0\in X$，$y_0 = f(x_0)\in Y$. 如果对于任意的 $\varepsilon>0$，都存在 $\delta>0$ 使得对于任意满足 $d_X(x, x_0) < \delta$ 的 $x\in X$，都有 $d_Y(f(x), y_0) < \varepsilon$，那么称 $f$ 在 $x_0$ 处\term{连续}. 如果 $f$ 在 $X$ 的每一点都连续，那么称 $f$ 是\term{连续映射}.
\end{definition}

在实数域上，倘若我们定义了一系列范数，比如 $p$ 范数，可以举出例子表明同一序列在不同范数下可能收敛到不同的极限，我们在习题中将涉及到这一点. 但是在有限维赋范线性空间上，这样的奇怪现象并不会出现，成为这个理论的基础是下面的 Weierstrass 定理：

\begin{theorem}{Weierstrass 定理}{}
    设 $S$ 为一个具有给定范数 $\Vert \cdot \Vert$ 的有限维的实或复的线性空间 $V$ 的一个紧子集，设 $f\colon S\to\mathbf{R}$ 是一个连续函数，那么 $f$ 在 $S$ 上可以取得最大值与最小值，这就是说，存在两个点 $x_1, x_2\in S$ 使得对于任意 $x\in S$ 都有 \[f(x_1)\leqslant f(x)\leqslant f(x_2).\]
\end{theorem}

对应的拓扑基础知识可以参考很多拓扑学或分析学的教材，这里不再赘述.

\begin{lemma}{}{}
    设 $\Vert \cdot\Vert$ 是定义在域 $\mathbf{F}$ 的赋范向量空间 $V$ 上的一个范数，$F$ 可以为实数域或者复数域，$m\geqslant 1$ 是一个给定的正整数，$x^{(1)}, x^{(2)}, \dots, x^{(m)}\in V$ 是给定的向量组，又对任意的 $z = (z_1, \cdots, z_m)^T \in \mathbf{F}^m$，可以定义 $x(z) = \sum\limits_{i=1}^{m}z_ix^{(i)}$，那么由 $x(z)$ 定义的函数 $g\colon \mathbf{F}^m\to \mathbf{R}$，
    \[g(z) = \Vert x(z)\Vert = \lVert \sum_{i=1}^{m}z_ix^{(i)}\rVert\] 是关于 Euclid 范数 $\Vert\cdot\Vert_2$ 一致连续的函数.
\end{lemma}

\begin{proof}
    设 $u = (u_1, \cdots, u_m)^T$，$v = (v_1, \cdots, v_m)^T$，那么
    \[\begin{aligned}
        \vert g(x(u)) - g(x(v))\vert
        &= \lvert \Vert x(u) \Vert - \Vert x(v) \Vert \rvert \leqslant \lVert x(u) - x(v)\rVert \\
        &= \lVert \sum_{i = 1}^{m} (u_i - v_i)x^{(i)}\rVert \leqslant \sum_{i = 1}^{m}\vert u_i - v_i\vert \Vert x^{(i)}\Vert \\
        &\leqslant \left(\sum_{i = 1}^{m}\vert u_i - v_i\vert^2\right)^{1/2}\left(\sum_{i = 1}^{m}\Vert x^{(i)}\Vert^2\right)^{1/2} =C\Vert u - v\Vert_2.
    \end{aligned}\]
    这个常数 $C = \left(\sum\limits_{i = 1}^{m}\Vert x^{(i)}\Vert^2\right)^{1/2}$ 仅仅与范数 $\Vert\cdot\Vert$ 以及给定的向量组 $x^{(1)}, x^{(2)}, \dots, x^{(m)}$ 有关，一定满足 $0 \leqslant C<\infty$，所以 $g$ 一定是一致连续的.
\end{proof}

上面引理中要求赋范线性空间 $V$ 不一定是有限维的，但是对于下面这个关键刻画，有限维这个条件就是不可缺少的：

\begin{theorem}{}{}
    设 $f_1$ 和 $f_2$ 是域 $\mathbf{F}$ 上的一个有限维向量空间 $V$ 上的一个实值函数，这里的 $\mathbf{R}$ 可以是实数域或者是复数域，设 $\mathcal{B} = \{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$ 是 $V$ 的一个基，对 $z = (z_1, z_2, \dots, z_m)^T\in \mathbf{F}^m$，定义 $x(z) = \sum\limits_{i=1}^{m}z_ix^{(i)}$. 若 $f_1$ 和 $f_2$ 为
    \begin{enumerate}
        \item 正定的：对所有的 $x\in V$ 都有 $f_i(x)\geqslant 0$，且 $f_i(x) = 0$ 当且仅当 $x = 0$；
        \item 齐次的：对所有的 $x\in V$ 和所有的 $\lambda\in\mathbf{F}$ 都有 $f_i(\lambda x) = \Vert \lambda\Vert f_i(x)$；
        \item 连续的：$f_1$ 在 $V$ 上关于 Euclid 范数 $\Vert\cdot\Vert_2$ 是连续的，或者说 $f_1(x(z))$ 在 $\mathbf{F}^m$ 上关于 Euclid 范数是连续的.
    \end{enumerate}
    那么就存在正常数 $C_m$ 和 $C_M$ 使得对于所有 $x\in V$ 都有 \[C_mf_1(x) \leqslant f_2(x) \leqslant C_Mf_1(x).\]
\end{theorem}

满足正定性、其次性与连续性的有限维实的或者复的向量空间上的实值函数被称为\term{准范数}，最经典的准范数当然是范数，上面这个定理告诉我们：在有限维实或者复的赋范线性空间上，所有的准范数都相差有限倍. 下面我们证明一下这个定理：

\begin{proof}
    在单位球面 $S = \{z\in \mathbf{F}^n \colon \Vert z\Vert_2 = 1\}$ 上定义 $h(z) = \dfrac{f_2(x(z))}{f_1(x(z))}$，$S$ 是 $\mathbf{F}^m$ 上关于 Euclid 范数的紧子集，正定性和连续性保证了 $h(z)$ 在 $S$ 上也是连续的，带有 Euclid 范数的 $\mathbf{F}^m$ 上的 Weierstrass 定理保证了 $h$ 在 $S$ 上可以取到有限的正最大值 $C_M$ 和正的最小值 $C_m$，所以对所有的 $z\in S$ 都有 \[C_m\leqslant h(z) = \frac{f_2(x(z))}{f_1(x(z))}\leqslant C_M.\]
    因为对每个非零的 $z\in\mathbf{F}^m$ 都有 $z/\Vert z\Vert_2\in S$，根据其次性得知 \[f_i(x(\frac{z}{\Vert z\Vert_2})) = f_i(\frac{1}{\Vert z\Vert_2}x(z)) = \frac{1}{\Vert z\Vert_2}f_i(x(z)),\]
    所以对于所有非零的 $z\in\mathbf{F}^m$ 都有 \[C_m f_1(x(z))\leqslant f_2(x(z))\leqslant C_M f_1(x(z)).\]
    该式对 $z = 0$ 也成立，因为 $f_1(0) = f_2(0) = 0$，每一个 $v\in V$ 都可以表示成 $\mathcal{B}$ 的线性组合的形式，所以自然可以表示成 $v = x(z)$ 的形式，这样孑军中的不等式对所有的 $x\in V$ 都成立.
\end{proof}

既然有限维线性空间上的范数都差不多，那么我们可以完全解决上面的问题：在有限维实或者复的赋范线性空间上，无论我们使用哪种范数，我们都可以得到相同的收敛性质. 也就是：

\begin{corollary}{}{}
    设 $\Vert \cdot\Vert_\alpha$ 和 $\Vert \cdot\Vert_\beta$ 是定义在域 $\mathbf{F}$ 的有限维赋范向量空间 $V$ 上的两个范数，这里的 $\mathbf{F}$ 可以是实数域或复数域，又如果存在 $\{x^{(i)}\}$ 是 $V$ 中一列给定的向量，$x\in V$ 是给定的向量，那么关于 $\Vert \cdot\Vert_\alpha$ 有 $\{x^{(i)}\}$ 收敛到 $x$ 当且仅当关于 $\Vert \cdot\Vert_\beta$ 有 $\{x^{(i)}\}$ 收敛到 $x$.
\end{corollary}

证明非常简单，直接略过则可. 在实或者复向量空间上，如果两个范数关于同一列向量序列的收敛性是等价的，那么这两个范数是\term{等价}的. 上面的论述告诉我们：\term{有限维实或者复向量空间上的范数都是等价的}，这就是我们在有限维赋范线性空间上研究范数的一个重要结论. 无限维线性空间上的结论就要复杂得多，读者可以阅读有关矩阵分析的书籍来了解更多.

线性映射终归也是一个“线性的函数”，所以自然可以考虑它的有界性和连续性，这里就需要利用范数来进行研究了：

\begin{definition}{赋范线性空间上的有界映射}{}
    设 $A$ 是赋范线性空间 $X$ 到赋范线性空间 $Y$ 的线性映射，如果存在常数 $M\leqslant 0$ 使得对于任意 $x\in X$，都有 \[\Vert Ax\Vert\leqslant M\Vert x\Vert,\]
    那么称 $A$ 是\term{有界映射}.
\end{definition}

\begin{theorem}{}{}
    对于线性赋范空间 $X$ 和 $Y$，从$X$ 到 $Y$ 的线性映射 $A$ 是有界映射的充分必要条件是 $A$ 将所有有界集映射为有界集.
\end{theorem}

\begin{proof}
    设 $A$ 将所有有界集映射为有界集，那么 $A$ 将单位球面 $S=\{x\in X \colon \Vert x\Vert=1\}$ 映射为一个有界集，那么存在常数 $M\leqslant 0$ 使得对于 $y\in S$ 有 $\Vert Ay\Vert\leqslant M$，当 $x = 0$ 的时候，$\Vert Ax\Vert \leqslant M\Vert x\Vert$自然成立，当 $x \neq 0$ 的时候，取 $y=\dfrac{x}{\Vert x\Vert}$，那么 \[\frac{\Vert Ax\Vert}{\Vert x\Vert} = \Vert A\frac{x}{\Vert x\Vert}\Vert = \Vert Ay\Vert \leqslant M.\] 这就说明了 $A$ 为有界映射.

    反过来，如果 $A$ 是有界映射，设 $B$ 是 $X$ 中的有界集，那么存在常数 $N$ 使得对于任意 $x\in B$ 都有 $\Vert x\Vert\leqslant N$，那么对于任意 $x\in B$，有 \[\Vert Ax\Vert\leqslant M\Vert x\Vert\leqslant MN,\]这就说明了 $A$ 将有界集映射为有界集.
\end{proof}

值得注意的是，利用 Zorn 引理可以证明：任何无限维赋范线性空间上必存在定义在全空间上的无界线性算子。

\begin{theorem}{赋范线性空间上的连续映射}{}
    设 $A$ 是赋范线性空间 $X$ 到赋范线性空间 $Y$ 上的线性映射，假设 $A$ 在某一点 $x_0\in X$ 处连续，那么 $A$ 是 $X$ 上的连续映射.
\end{theorem}

\begin{proof}
    对任意一点 $x\in X$，设 $\{x_n\}$ 是 $X$ 中的一个序列，满足 $\lim\limits_{n\to\infty}x_n = x$，那么 $\lim_{n\to\infty}x_n - x + x_0 = x_0$，由假设 $A$ 在 $x_0$ 处连续，所以 \[\lim\limits_{n\to\infty}A(x_n - x + x_0) = A(x_0),\]
    所以 \[\lim\limits_{n\to\infty}Ax_n = Ax.\]
    这就说明了 $A$ 在 $x$ 处连续. 由 $x$ 的任意性可知 $A$ 在 $X$ 上是连续映射.
\end{proof}

\begin{theorem}{有界性和连续性的等价性}{}
    从赋范线性空间 $X$ 到赋范线性空间 $Y$ 上的线性映射 $A$ 是有界映射的充分必要条件是 $A$ 是连续映射.
\end{theorem}

更进一步的，如果 $A\in \mathcal{L}(\mathbf{R}^n, \mathbf{R}^m)$ 那么可以直接得到 $A$ 是有界且一致连续的. 当我们取 $\mathbf{R}^n$ 上的一组标准基 $\{e_1, e_2, \cdots, e_n\}$，设 $x\in \mathbf{R}^n$，$\Vert x\Vert \leqslant 1$，那么 $x$ 就可以表示为 $x = \ds\sum_{i=1}^{n} x_ie_i$，$\vert x_i \vert \leqslant 1$. 于是
\[\Vert Ax\Vert = \Vert \sum_{i=1}^{n} x_iAe_i\Vert \leqslant \sum_{i=1}^{n}\vert x_i\vert \Vert Ae_i\Vert \leqslant \sum_{i=1}^{n}\Vert Ae_i\Vert.\]
所以\[\Vert A\Vert \leqslant \sum_{i=1}^{n}\Vert Ae_i\Vert < \infty.\]
并且因为当 $x, y\in \mathbf{R}^n$ 时，$\Vert Ax - Ay \Vert \leqslant \Vert A\Vert\Vert x-y\Vert$，所以 $A$ 是一致连续的.

\begin{theorem}{}{}
    设 $\Omega$ 为 $\mathbf{R}^n$ 上所有可逆线性算子的集合.
    \begin{enumerate}
        \item 若 $A\in \Omega$，$B\in \mathcal{L}(\mathbf{R}^n)$，并且 \[\Vert B-A\Vert\cdot\Vert A^{-1}\Vert < 1,\]那么 $B\in \Omega$；
        \item $\Omega$ 是 $\mathcal{L}(\mathbf{R}^n)$ 的开集，映射 $A\to A^{-1}$ 是 $\Omega$ 上的连续映射.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item 令 $\Vert A^{-1}\Vert = \dfrac{1}{\alpha}$，$\Vert B-A\Vert = \beta$，那么 $\beta < \alpha$，对于每一个 $x\in \mathbf{R}^n$，
        \[\begin{aligned}
            a\Vert x\Vert &= \alpha\Vert A^{-1}Ax\Vert \leqslant \alpha \Vert A^{-1}\Vert \cdot \Vert Ax \Vert \\
            &= \Vert Ax\Vert \leqslant \Vert (A - B)x\Vert + \Vert Bx\Vert \leqslant \beta\Vert x\Vert + \Vert Bx\Vert,
        \end{aligned}\]
        因此 \[(\alpha - \beta)\Vert x\vert \leqslant \Vert Bx\Vert,\enspace \forall x\in \mathbf{R}^n.\]
        因为 $\alpha - \beta > 0$，所以只要 $x\neq 0$，就有 $Bx\neq 0$，所以 $B$ 就是一一映射，也一定是可逆映射. 这对所有满足 $\vert B - A\Vert < \alpha$ 的 $B$ 都成立，这就完成了证明并且证明了 $\Omega$ 是开集；
        \item 在 $(\alpha - \beta)\Vert x\vert \leqslant \Vert Bx\Vert$ 将 $x$ 换成 $B^{-1}y$ 可以得到 \[(\alpha - \beta)\Vert B^{-1}y\vert \leqslant \Vert BB^{-1}y\Vert = \Vert y\Vert,\enspace y\in \mathbf{R}^n,\]
        这说明 $\Vert B^{-1}\Vert \leqslant \dfrac{1}{\alpha - \beta}$，于是根据恒等式 $B^{-1} - A^{-1} = B^{-1}(A - B)A^{-1}$ 与 $\Vert AB\Vert \leqslant \Vert A\Vert \Vert B\Vert$ 可以得到 \[\Vert B^{-1} - A^{-1}\Vert \leqslant \Vert B^{-1}\Vert\Vert A - B\Vert\Vert A^{-1}\Vert \leqslant \dfrac{\beta}{\alpha(\alpha - \beta)}.\]
        当 $B\to A$ 时，$\beta\to 0$，这就证明了 $A\to A^{-1}$ 是连续映射.
    \end{enumerate}
\end{proof}

\section{多元函数微分学}

为了得到定义在 $\mathbf{R}^n$ 上的函数的导数与微分的定义，回顾一下一元函数微分学的一些内容，看看怎样把 $n=1$ 的情形解释一下，进而可以自然地推广到 $n>1$ 的情况.

设$f$是定义在$(a,b)\subset \mathbf{R}$上的实值函数，$x_0\in (a,b)$ 是一个给定的点，那么如果极限 \[\lim_{h\to 0}\frac{f(x_0 + h)-f(x_0)}{h}.\]
存在，那么 $f$ 在 $x_0$ 处的导数 $f'(x_0)$ 就定义为这个极限的值. 我们经常谈到导数的几何意义：导数反应了函数在某一点处的变化率，也就是在该点处的切线斜率. 将切线抽象成一个线性映射，我们就得到了\term{微分}，其定义是满足 \[f(x_0 + v) = f(x_0) + A(v) + o(\Vert v\Vert),\enspace \Vert v\Vert\to 0.\]
的线性映射 $A$. 导数在高维的推广是\term{方向导数}，而微分的定义自然是一脉相承的.

\begin{definition}{多元函数的导数与微分}{}
    给定函数 $f: \mathbf{R}^n\to\mathbf{R}$ 与给定的 $x_0, v\in\mathbf{R}^n$，如果极限 \[\lim_{h\to 0}\frac{f(x_0 + hv) - f(x_0)}{\Vert hv\Vert}\] 存在，那么称 $f$ 在 $x_0$ 处沿 $v$ 的\term{方向导数}存在，并且记作 $\dfrac{\partial f}{\partial v}(x_0)$. 如果存在一个线性映射 $A\in \mathcal{L}(\mathbf{R}^n, \mathbf{R})$，$x\in \mathbf{R}^n$，
    \[\lim\limits_{\Vert x \Vert \to0}\frac{\vert f(x_0 + x) - f(x_0) - Ax \vert}{\Vert h \Vert} = 0.\]

\end{definition}

思考一下从一元函数到多元函数的推广过程：在一元函数，我们研究的主要是导数，导数的实际意义是函数的变化率，一元函数的自变量只有一个，表示自变量的点只能在直线上变动，移动的方向也只有左右两个方向；而当我们想在多元函数上讨论函数的变化率的时候，我们忽然发现，表示自变量的点可以在一个区域内任意移动了，不仅可以移动距离，而且可以按任意方向移动同一段距离. 因此函数的变化不仅与移动的距离有关，还与移动的方向有关，所以我们尝试考虑某一个方向上函数的变化率，当方向被限制在坐标轴方向的时候，我们就得到\term{偏导数}，当我们考虑某一个方向 $v$ 上的变化率的时候，我们就得到\term{方向导数}. 可是我们不想仅限于此，我们注意到可以将方向 $v$ 分解为坐标轴方向的向量的和，而我们恰恰有函数在坐标轴方向上的变化率——偏导数，所以我们可以将函数在方向 $v$ 上的变化率表示为函数在坐标轴方向上的变化率的线性组合，这也就是微分的想法.

\begin{theorem}{方向导数与微分}{}
    对函数 $f: \mathbf{R}^n\to \mathbf{R}$，设 $f$ 在 $x_0$ 处可微，那么 $f$ 在 $x_0$ 处的方向导数都存在，并且有\[\frac{\partial f}{\partial u}(x_0) = \mathrm{d}f(x_0)(u).\]
    并且 $f$ 在 $x_0$ 的偏导数 $f_{x_i}(x_0)$ 都存在，记 $f$ 在 $x_0$ 处的\term{梯度}为 \[\nabla f(x_0) = (f_{x_1}(x_0), f_{x_2}(x_0), \cdots, f_{x_n}(x_0)),\]
    那么就有 \[\mathrm{d}f(x_0)(u) = \nabla f(x_0)\cdot u.\]这就表明了\textrm{梯度其实是微分的矩阵表示}.
\end{theorem}

既然函数就是两个集合之间的映射，迄今为止我们考虑的一直是$\mathbf{R}^n$上的实值函数，既然线性映射都可以是从$\mathbf{R}^n$到$\mathbf{R}^m$的，那么函数当然也可以这样，这就是\term{向量值函数}.

向量值函数只是只是对于函数到达空间的简单推广，甚至在一般的度量空间之间的映射的视角，这仅仅只是一个特例而已. 我们知道，对于一元函数与多元函数在某一点处的微分其实是一个线性映射 $L$，使得其是在这点处的最佳线性逼近. 换句话说就是\[f(x) - f(x_0) = L(x-x_0) + o(\Vert x-x_0\Vert)\quad (x\to x_0).\]我们按照这样的思路，可以定义多元向量值函数的微分：

\begin{definition}{向量值函数的微分}{}
    设 $D\subset\mathbf{R}^n$ 是开集，$f: D\to\mathbf{R}^m$ 是一个向量值的多元函数函数，$x_0\in D$. 如果存在一个线性映射 $L\in \mathcal{L}(\mathbf{R}^n, \mathbf{R}^m)$ 使得在 $x_0$ 附近成立\[f(x) - f(x_0) = L(x-x_0) + o(\Vert x-x_0\Vert)\quad (x\to x_0),\]那么称 $f$ 在 $x_0$ 处\term{可微}，并且称 $L$ 为 $f$ 在 $x_0$ 处的\term{微分}，记作 $\mathrm{d}f(x_0)$.
\end{definition}

从经验上看，向量值函数与其微分的出现是自然而必然的，对于一个一般的多元函数 $f: \mathbf{R}^n\to \mathbf{R}$，我们定义了它的微分是一个线性映射 $\mathrm{d}f\in \mathcal{L}(\mathbf{R}^n, \mathbf{R})$，按照一维的理论，对一次导数求导就得到了二次导数，所以我们应该对 $x \mapsto \mathrm{d}f$ 求微分，而这实际上是一个 $\mathbf{R}^n\to \mathcal{L}(\mathbf{R}^n,\mathbf{R})\simeq \mathbf{R}^n$ 的函数. 你看，这就是向量值函数！所以我们自然要定义向量值函数与其微分.

再者，从换元的角度，对于一个二元函数 $f(x, y)$，我们可以随便找一个换元 $x = x(u, v)$ 与 $y = y(u, v)$，那么 $f(x, y)$ 就变成了 $f(x(u, v), y(u, v))$，将换元后的函数视作一个复合函数 $f\circ \varphi$，那么 $\varphi$ 必须是从 $\mathbf{R}^2$ 到 $\mathbf{R}^2$ 的，也就是 $\varphi(u, v) = (x(u, v), y(u, v))$. 这就是向量值函数的另一个实例.

从一元函数到多元函数，对微分的定义其实迈上了一个新的台阶，原因其实是自变量的每一个分量“杂糅”在一起，我们需要单独将每一个分量“拎出来”单独讨论，所以这就是偏导数与全微分的来源. 但是对于向量值函数而言，它的每一个分量并没有什么特别强的联系——这是因为我们有了坐标，并且这严重依赖于坐标系的选取——所以我们可以将从 $\mathbf{R}^n$ 到 $\mathbf{R}^m$ 的向量值函数拆成 $m$ 个分量，每一个分量都是从 $\mathbf{R}^n$ 到 $\mathbf{R}$ 的函数. 在这个角度上，我们可以直接但是不严谨地得出：向量值函数 $f$ 在 $x_0$ 处可微当且仅当其每一个分量 $f_i$ 在 $x_0$ 处都可微，并且其微分为
\[
    \mathrm{d}f(x_0) = Jf(x_0) =
    \begin{pmatrix} \nabla f_1(x_0) \\ \nabla f_2(x_0) \\ \vdots \\ \nabla f_n(x_0) \end{pmatrix} =
    \begin{pmatrix}
        \dfrac{\partial f_1}{\partial x_1}(x_0) &\dfrac{\partial f_1}{\partial x_2}(x_0) & \cdots & \dfrac{\partial f_1}{\partial x_n}(x_0) \\
        \dfrac{\partial f_2}{\partial x_1}(x_0) & \dfrac{\partial f_2}{\partial x_2}(x_0) & \cdots & \dfrac{\partial f_2}{\partial x_n}(x_0) \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial f_m}{\partial x_1}(x_0) & \dfrac{\partial f_m}{\partial x_2}(x_0) & \cdots & \dfrac{\partial f_m}{\partial x_n}(x_0)
    \end{pmatrix}.
\]
其中矩阵 $Jf(x_0)$ 是 $f$ 在 $x_0$ 的微分的矩阵表示，被称为\textrm{Jacobi 矩阵}. 这非常符合直觉，严谨的证明可以参考于品教授的《数学分析讲义》，这里只截取对应的定理：

\begin{theorem}{微分的计算}{}
    假设 $V = \mathbf{R}^n$ 和 $W = \mathbf{R}^m$，我们分别取坐标系 $\{x_i\}_{i = 1, 2, \cdots, n}$ 和 $\{y_j\}_{j = 1, 2, \cdots, m}$，考虑函数 \[f: V\to W,\enspace x\mapsto f(x) = (f_1(x_1, x_2, \cdots, x_n), \cdots, f_m(x_1, x_2, \cdots, x_n)).\]
    那么 $f$ 在 $x_0$ 处可微当且仅当每一个 $f_i$ 在 $x_0$ 处都可微，并且微分 $\mathrm{d}f(x_0)\in \mathcal{L}(\mathbf{R}^n, \mathbf{R}^m)$ 的矩阵表示为
    \[\left(\frac{\partial f_i}{\partial x_j}(x_0)\right)_{\begin{subarray}  \enspace i = 1, 2, \cdots, m \\ j = 1, 2, \cdots, n\end{subarray}}.\]
\end{theorem}

同样，链式法则在多元函数的微分中也是成立的：

\begin{theorem}{链式法则}{}
    设 $D$ 和 $\Delta$ 分别为 $\mathbf{R}^n$ 和 $\mathbf{R}^m$ 上的开集，$f: D\to \mathbf{R}^m$ 和 $g: \Delta\to \mathbf{R}^l$ 为向量值函数，且 $f(D)\subset \Delta$. 如果 $f$ 在 $x_0\in D$ 中可微，$g$ 在 $y_0 = f(x_0)$ 处可微，那么复合函数 $h = g\circ f$ 在 $x_0$ 处可微，并且有\[Jh(x_0) = Jg(y_0)Jf(x_0).\]

    对应映射的复合与微分（在线性的层次上）可以拿下面两个交换图表示：
    \begin{center}
        \begin{tikzcd}
            \Omega_1 \arrow[r, "f"] \arrow[rd, "g\circ f" description]
                &\Omega_2 \arrow[d, "g"]\\
                &\Omega_3
        \end{tikzcd}
        \qquad
        \begin{tikzcd}
            \mathbf{R}^{m_1} \arrow[r, "\mathrm{d}f(x_1)"] \arrow[rd, "\mathrm{d}(g\circ f)(x_1)" description]
                &\mathbf{R}^{m_2} \arrow[d, "\mathrm{d}g(f(x_1))"]\\
                &\mathbf{R}^{m_3}
        \end{tikzcd}
    \end{center}
\end{theorem}

证明方法与一维的如出一辙，这里也不多赘述.

\begin{corollary}{反函数的微分}{}
    给定区域 $D\subset \mathbf{R}^n$ 和 $\Delta \subset \mathbf{R}^m$ 与向量值函数 $f: D\to \Delta$，如果 $f$ 是一一映射并且其逆映射 $f^{-1}: \Delta\to D$ 是可微的，那么有
    \begin{enumerate}
        \item[(1)] $m = n$；
        \item[(2)] $Jf(x)$ 是可逆的，并且有 \[J(f^{-1})(y) = [\left.Jf(x)\right|_{x = f^{-1}(y)}]^{-1}.\]
    \end{enumerate}
\end{corollary}

\begin{proof}
    我们令 $\Omega = D$，$g = f^{-1}: \Omega\to\Delta$，那么有 $g \circ f = \mathrm{id}_{\Omega}$，根据链式法则有 \[\mathrm{Id}_{\Omega} = I_{n} = J(g\circ f)(x_0) = Jg(y_0)\cdot Jf(x_0),\]
    反过来用 $f^{-1}$ 替代 $f$ 可得另一侧\[\mathrm{Id}_{\Delta} = I_m = J(f\circ g)(y_0) = Jf(x_0)\cdot Jg(y_0).\]
    根据线性代数的知识，很容易可以看出 $Jf(x)$ 和 $Jg(g)$ 都需要是一个方阵，这就是 (1). 并且也足以说明 $Jf(x)$ 是可逆的，也就是$Jg(y) = (Jf(x))^{-1}.$
\end{proof}

与一元函数的微分中值定理相似，我们也有多元函数的微分中值定理：

\begin{theorem}{微分中值定理}{}
    设 $D\subset \mathbf{R}^n$ 为凸域，函数 $f:D\to \mathbf{R}$ 在 $D$ 中处处可微，则任给 $x,y\in D$，存在 $\theta\in (0, 1)$ 使得\[f(y) - f(x) = \nabla f(\xi)\cdot(y-x),\enspace \xi=\theta x + (1-\theta)y.\]
\end{theorem}

\begin{proof}
    和一元函数的证明是类似的，令 $\sigma(t) = tx + (1 - t)y$，由 $D$ 为凸域可知当 $t\in [0, 1]$ 的时候 $\sigma(t)\in D$，那么 $\varphi = f\circ \sigma$ 是一个一元函数，对其使用微分中值定理可知，存在 $\theta\in (0, 1)$ 使得 \[\varphi(1) - \varphi(0) = \varphi'(\theta).\]
    由链式法则，我们有 \[\varphi(1) - \varphi(0) = \nabla f(\xi)\cdot\sigma'(\theta) = \nabla f(\xi)\cdot(x - y).\]
    其中 $\xi = \sigma(\theta) = \theta x + (1 - \theta)y$，由 $f(x) = \varphi(1)$，$f(y) = \varphi(0)$，我们就得到了结论.
\end{proof}

一个很自然的问题就是微分中值定理可不可以推广到向量值函数？设 $D\subset \mathbf{R}^n$ 为凸域，$f: D\to \mathbf{R}^m$ 是一个向量值函数，$x, y\in D$，对 $f$ 的每一个分量 $f_i$ 应用微分中值定理可以得到\[f_i(x) - f_i(y) = \nabla f_i(\xi_i)\cdot(x-y),\]其中 $\xi_i\in D$，注意到这些 $xi_i$ 未必相同，比如对于函数 $f: \mathbf{R}\to\mathbf{R}^2,\enspace f(t) = (t^2, t^3)$，取 $x = 1$，$y = 0$，通过计算得知 $\xi_1 = \dfrac{1}{2}$，$\xi_2 = \pm\dfrac{1}{\sqrt{3}}$，因此 $\xi_1 \neq \xi_2$. 这就表明我们不能指望着 $f(x) - f(y) = Jf(\xi)(x-y)$ 对于某一个 $\xi$ 成立. 但是我们却可以有另一个不错的估计，这就是\term{拟微分中值定理}.

\begin{theorem}{拟微分中值定理}{}
    设$D\subset \mathbf{R}^n$ 为凸域，$f: D\to \mathbf{R}^m$ 在 $D$ 中处处可微，则任给 $x, y\in D$，存在 $\xi\in D$ 使得\[\Vert f(x) - f(y)\Vert \leqslant \Vert Jf(\xi)\Vert\cdot \Vert x - y\Vert.\]
\end{theorem}

证明的基本想法是对 $f$ 的分量的线性组合应用微分中值定理，然后进行放缩.

\begin{proof}
    不妨设 $f(x) \neq f(y)$，任意取定 $\mathbf{R}^m$ 中的单位向量 $u = (u_1, u_2, \cdots, u_m)$，记\[g = u\cdot f = \sum_{i = 1}^{m}u_if_i.\]
    则 $g$ 为 $D$ 中的可微实值函数，并且对 $x, y$ 存在 $\xi\in D$ 使得 \[g(x) - g(y) = \nabla g(\xi)\cdot (x - y).\]
    且 \[\nabla g(\xi) = \sum_{i = 1}^{m}u_i\nabla f_i(\xi).\]
    利用 Cauchy-Schwarz 不等式，有 \[\Vert \nabla g(\xi)\Vert \leqslant \sum_{i = 1}^m\vert u_i\vert \Vert \nabla f_i(\xi)\Vert \leqslant \left(\sum_{i = 1}^m\vert u_i\vert^2\right)^{1/2}\left(\sum_{i = 1}^m\Vert \nabla f_i(\xi)\Vert^2\right)^{1/2} = \Vert Jf(\xi)\Vert.\]
    由 $g(x) - g(y) = u\cdot (f(x) - f(y))$，我们有 \[\vert u \cdot [f(x) - f(y)]\vert = \vert g(x) - g(y) \vert \leqslant \Vert \nabla g(\xi)\Vert \cdot \Vert x - y\Vert \leqslant \Vert Jf(\xi)\Vert\cdot \Vert x - y\Vert.\]
    取 $u = \dfrac{f(x) - f(y)}{\Vert f(x) - f(y)\Vert}$ 就完成了证明.
\end{proof}

拟微分中值定理告诉我们，雅各布矩阵的范数可以用来估计向量值函数的改变量. 下面的结果给出了向量值函数与其线性化映射之间的误差估计.

\begin{corollary}{}{}
    设 $D\subset \mathbf{R}^n$ 为开集，$f: D\to \mathbf{R}^m$ 在 $D$ 中处处可微，且 $Jf(x)$ 关于 $x$ 连续，如果 $C\subset D$ 为紧凸集，则任给 $\varepsilon>0$，存在 $\delta>0$ 使得对于任意 $u, v\in C$，只要 $\Vert u-v\Vert < \delta$，就有\[\Vert f(u) - f(v) - Jf(v)(u-v)\Vert \leqslant \varepsilon\Vert u-v\Vert.\]
\end{corollary}

\begin{proof}
    由于欧氏空间的紧子集都是有界闭的，所以我们知道 $Jf$ 在 $C$ 上一致连续，所以任给 $\varepsilon>0$，存在 $\delta>0$ 使得只要 $u',u\in C$ 且 $\Vert u'-u\Vert < \delta$，就有\[\Vert Jf(u') - Jf(u)\Vert < \varepsilon.\]
    暂时固定 $u\in C$，当 $x\in D$ 时，记 $F(x) = f(x) - f(u) - J(u)(x - u)$，则 $JF(x) = Jf(x) - Jf(u)$，当 $v\in C$ 时，使用拟微分中值定理，有
    \[\Vert F(v)\Vert = \Vert F(v) - F(u)\Vert \leqslant \Vert JF(\xi)\Vert\cdot \Vert v - u\Vert = \Vert Jf(\xi) - Jf(u)\Vert\cdot \Vert v - u\Vert.\]
    其中 $\xi = \theta u + (1-\theta)v$，$\theta\in (0, 1)$，由于 $Jf$ 在 $C$ 上一致连续，所以只要 $\Vert v - u\Vert < \delta$，就有 $\Vert \xi - u\Vert = \theta \Vert v - u\Vert < \delta$，所以\[\Vert F(v)\Vert = \Vert f(v) - f(u) - Jf(u)(v - u)\Vert \leqslant \varepsilon\Vert v - u\Vert.\]
\end{proof}

既然我们可以对一个函数的反函数求微分，回想起对于一元函数，如果它可微并且导数处处非零，那么该函数存在反函数并且反函数可微. 那么一个自然的问题就是：在什么样的条件下，多元向量值函数存在反函数并且反函数可微呢？这就是\term{反函数定理}. 在考虑反函数定理之前，我们先看一个小小的比喻.

\begin{example}{}{}
    设 $A$ 是一个 $n$ 阶方阵，如果 $\Vert A\Vert < 1$，则 $I_n - A$ 可逆.
\end{example}

\begin{proof}
    设 $u\in \mathbf{R}^n$，如果 $(I_n - A)u = 0$，则 $\Vert u\Vert = \Vert Au\Vert \leqslant \Vert A\Vert \cdot \Vert u\Vert < \Vert u\Vert$，那么 $u$ 只能等于 $0$，所以 $I_n - A$ 是单射，所以其可逆.
\end{proof}

以分析学的视角看：给定 $v\in \mathbf{R}^n$，解方程 \[(I_n - A)x = v,\enspace\text{亦即}\enspace x = Ax+u.\]
我们先考虑映射 $\varphi(x) = Ax+v$，当 $x,y\in \mathbf{R}^n$ 时，\[\Vert \varphi(x) - \varphi(y)\Vert = \Vert A(x - y)\Vert\leqslant \Vert A\Vert\cdot \Vert x - y\Vert.\]

这个性质很不错，回忆一下\term{压缩映射原理}：

\begin{theorem}{压缩映射原理}{}
    设 $(X,d)$ 是一个完备的度量空间，$C$ 是一个非空闭子集，$f: C\to C$ 是一个压缩映射，即存在常数 $0\leqslant k < 1$ 使得对于任意 $x, y\in C$ 都有 \[d(f(x), f(y))\leqslant kd(x, y),\]那么 $f$ 在 $C$ 上有唯一的不动点. 即存在唯一的 $x_*\in C$ 使得 $f(x_*) = x_*$.
\end{theorem}

\begin{proof}
    任取 $a_0 \in C$，当 $n\geqslant 1$ 时，利用 $a_n = f(a_{n-1})$ 可以递归定义 $C$ 中的数列 $\{a_n\}$，我们来说明 $\{a_n\}$ 为 Cauchy 列. 根据题设，存在 $0\leqslant L <1$，使得当 $n\geqslant 1$ 时，\[d(a_{n+1}, a_n) = d(f(a_n), f(a_{n-1}))\leqslant Ld(a_n, a_{n-1}).\]
    由上式与归纳法可知 $d(a_n,a_{n-1})\leqslant L^{n-1}d(a_1, a_0)$ 对 $n\geqslant 1$ 均成立，当 $m > n$ 时，有
    \[\begin{aligned}
        d(a_m, a_n) & \leqslant d(a_m, a_{m-1}) + d(a_{m-1}, a_{m-2}) + \cdots + d(a_{n+1}, a_n) \\
        & \leqslant (L^{m-1} + L^{m-2} + \cdots + L^{n})d(a_1, a_0) \\
        & \leqslant \frac{L^n}{1-L}d(a_1, a_0).
    \end{aligned}\]
    由于 $L < 1$，所以当 $n\to\infty$ 时，$L^n\to 0$，所以 $\{a_n\}$ 为 Cauchy 列，其极限记为 $a_*$，则 $a_*\in C$，由 $a_n = f(a_{n-1})$ 与 $f$ 的连续性可知 $f(a_*) = a_*$.

    下面证明唯一性：若另有 $a\in C$ 使得 $f(a) = a$，则\[d(a, a_*) = d(f(a), f(a_*))\leqslant Ld(a, a_*),\]由于 $0\leqslant L < 1$，所以 $d(a, a_*) = 0$，即 $a = a_*$.
\end{proof}

所以这是一个压缩映射，因此 $\varphi$ 有唯一的不动点，即存在唯一的 $x_*\in \mathbf{R}^n$ 使得 $x_* = Ax_* + v$，即 $(I_n - A)x_* = v$，所以 $I_n - A$ 为单射，即可逆.

对于恒同映射 $I_n$ 来讲，范数小于 $1$ 的映射相对于它是一个\textrm{微小扰动}，上面的例子告诉我们，对于一个像恒同映射这样的可逆映射施加一个微小扰动得到的映射是可逆的. 在一般的向量值函数中，微小扰动的例子丰富极了，因为每一个在某一点 $x_0$ 可微的向量值函数在该点都可以将其线性化，也就是分解成 $L(x - x_0) + o(\Vert x - x_0\Vert)$，将后一项视作微小扰动 $\varepsilon(x - x_0)$ ，所以我们可以猜测：如果向量值函数的微分在某一点 $x_0$ 可逆，那么存在 $x_0$ 的邻域 $U$ 与 $y_0 = f(x_0)$ 的邻域 $V$ 使得 $\left.f\right|_U: U\to V$ 是可逆的. 不过我们需要对这个函数添加一定的可微性条件. 一个向量值函数是 $C^k$ 的是指其每个分量都是一个 $k$ 次连续可微函数. 这就是\term{反函数定理}的内容.

\begin{theorem}{反函数定理}{}
    设 $D\subset \mathbf{R}^n$ 为开集，$f: D\to\mathbf{R}^n$ 为 $C^{k}$ 的映射 $(k\geqslant 1)$，$x_0\in D$. 如果 $\det Jf(x_0)\neq 0$，那么存在 $x_0$ 的邻域 $U\subset D$ 与 $f(x_0)$ 的邻域 $V\subset D$，使得 $\left.f\right|_U: U\to V$ 是可逆的，并且其逆也是 $C^k$ 的.
\end{theorem}

\begin{proof}

    由于平移不改变函数的可微性与可逆性，所以我们不失一般性地设 $(x_0, y_0) = (0, 0)$，记 $L$ 为 $f$ 在 $x_0 = 0$ 处的微分，则 $L$ 可逆，那么根据链式法则，$L^{-1}\circ f$ 在 $x_0$ 处的微分是恒同映射. 如果欲证结论对于 $L^{-1}\circ f$ 成立，那么套上一个可逆的线性映射 $L$ 之后，对于 $f$ 也一定成立. 因此我们可以直接就假设 $Jf(x_0) = I_n$.

    在 $x_0 = 0$ 附近，$g$ 是恒同映射的小扰动：\[g(x) = f(x) - x,\enspace Jg(0) = 0.\]
    扰动项 $g$ 自然也是 $C^k$ 的，因此存在 $\delta > 0$，使得\[\Vert Jg(x)\Vert \leqslant \frac{1}{2},\enspace \forall x\in \overline{B(0, \delta)}\subset D.\]
    由拟微分中值定理可知：\[\Vert g(x_1) - g(x_2)\Vert \leqslant \frac{1}{2}\Vert x_1 - x_2\Vert,\enspace \forall x_1, x_2\in \overline{B(0, \delta)}.\]
    给定 $y\in B(0, \frac{\delta}{2})$，我们在 $B(0, \delta)$ 中解方程 \[f(x) = y,\enspace\text{亦即}\enspace x = y - g(x).\]
    特别地，取 $x_2 = 0$，那么 \[\Vert g(x_1) \Vert \leqslant \frac{1}{2}\Vert x_1\Vert.\]
    记 $\varphi(x) = y - g(x)$，当 $x\in \overline{B(0, \delta)}$ 时\[\Vert \varphi(x)\Vert \leqslant \Vert y\Vert +\Vert g(x)\Vert < \frac{\delta}{2} + \frac{1}{2}\Vert x\Vert \leqslant \delta.\]
    这说明 $\varphi(\overline{B(0, \delta)})\subset B(0, \delta)$. 当 $x_1, x_2\in \overline{B(0, \delta)}$ 时\[\Vert \varphi(x_1) - \varphi(x_2) \Vert = \Vert g(x_2) - g(x_1)\Vert \leqslant \frac{1}{2}\Vert x_2 - x_1\Vert.\]
    根据压缩映射原理，$f(x) = y$ 或 $x = y - g(x)$ 在 $\overline{B(0, \delta)}$ 中有唯一解，记作 $x_y$. 且 $\Vert x_y\Vert \leqslant \delta$，也就是 $x_y\in B(0, \delta)$. 令\[U = f^{-1}(B(0, \frac{\delta}{2}))\cap B(0, \delta),\enspace V = B(0, \frac{\delta}{2}),\]
    则我们已经证明了 $\left.f\right|_U: U\to V$ 是一一映射，其逆映射 $h(y) = x_y$ 满足 \[y - g(h(y)) = h(y)\]
    下面逐步证明 $h$ 是 $C^k$ 的.

    \begin{enumerate}
        \item[(1)] $h: V\to U$ 是连续映射：当 $y_1,y_2\in V$ 时，\[\Vert h(y_1) - h(y_2)\Vert \leqslant \Vert y_1 - y_2 \Vert +\Vert g(h(y_1)) - g(h(y_2))\Vert \leqslant \Vert y_1 - y_2\Vert + \frac{1}{2}\Vert h(y_1) - h(y_2)\Vert,\]
        这说明 $\Vert h(y_1) - h(y_2)\Vert \leqslant 2\Vert y_1 - y_2\Vert,\enspace \forall y_1, y_2\in V$. 所以 $h$ 是连续的.
        \item[(2)] $h: V\to U$ 是可微映射：设 $y_0\in V$，则对 $y\in V$ 有
            \[\begin{aligned}
                h(y) - h(y_0) &= (y - y_0) - [g(h(y)) - g(h(y_0))]\\
                &= (y - y_0) - Jg(h(y_0))(h(y) - h(y_0)) + o(\Vert h(y) - h(y_0)\Vert).
            \end{aligned}\]
            利用 $Jf = I_n = Jg$ 与 (1)，上式可以改写为\[Jf(h(y_0))\cdot(h(y) - h(y_0)) = y - y_0 + o(\Vert y - y_0\Vert),\]
            所以 \[h(y) - h(y_0) = Jf(h(y_0))^{-1}\cdot(y - y_0) + o(\Vert y - y_0\Vert)\]
            因而 $h$ 在 $y_0$ 处可微.
        \item[(3)] $h: V\to U$ 是 $C^k$ 的：由 (2) 有 $Jh(y) = Jf(h(y))^{-1},\enspace \forall y\in V$. 由于 $f$ 是 $C^k$ 的知 $Jf$ 是 $C^{k-1}$ 的. 由上知 $Jh$ 连续，所以 $h$ 是 $C^2$ 的，以此类推就可得到 $h$ 是 $C^k$ 的.
    \end{enumerate}

    这样就完成了证明.
\end{proof}

利用反函数定理，我们甚至可以利用它研究隐函数. 所谓“隐”函数，对于一个函数 $f(x, y)$ 来说，就是方程 $f(x, y) = c$ 实际上隐含的将 $y$ 定义为 $x$ 的函数，\term{隐函数定理}具体将这个函数构造了出来，下面是一个很有启发性的例子：

\begin{example}{}{}{\label{ex:25:隐函数}}
    设 $f:\mathbf{R}^2\to\mathbf{R}^2$ 为 $C^{k}$ 函数，$k\geqslant 1$， $\dfrac{\partial f}{\partial y}(x^0, y^0)\neq 0$，在 $(x^0, y^0)$ 附近解方程 $f(x, y) = f(x_0, y_0)$.
\end{example}

\begin{solution}
    显然 $(x_0, y_0)$ 是方程的解，利用反函数定理，我们可以在 $(x_0, y_0)$ 附近找到别的解，因此构造函数\[F:\mathbf{R}^2\to \mathbf{R}^2,\enspace F(x, y) = (x, f(x, y)),\]
    在 $(x^0, y^0)$ 处，有 \[\det JF(x^0, y^0) = \begin{vmatrix}
        1 & 0 \\
        \dfrac{\partial f}{\partial x}(x^0, y^0) & \dfrac{\partial f}{\partial y}(x^0, y^0)
    \end{vmatrix} = \dfrac{\partial f}{\partial y}(x^0, y^0)\neq 0.\]
    由反函数定理，在 $(x^0, y^0)$ 附近 $F$ 为可逆映射. 于是当 $x$ 在 $x^0$ 附近时，记 \[F^{-1}(x, f(x^0, y^0)) = (\varphi(x), \psi(x)),\]
    则 $\varphi(x)$ 与 $\psi(x)$ 均为 $C^k$ 函数. 根据 $F$ 的定义可得 \[(\varphi(x), f(\varphi(x), \psi(x))) = (x, f(x^0, y^0)).\]
    这说明 $\varphi(x) = x$，并且 $f(x, \psi(x)) = f(x^0, y^0)$，所以 $\psi(x)$ 就是我们要找的隐函数. 对 $x$ 求导还可以得到 \[\frac{\partial f}{\partial x}(x, \psi(x)) + \frac{\partial f}{\partial y}(x, \psi(x))\psi'(x) = 0,\]
    从而有 \[\psi'(x) = -[\frac{\partial f}{\partial y}(x, \psi(x))]^{-1}\frac{\partial f}{\partial x}(x, \psi(x)).\]
    这就给出了对 $y = \psi(x)$ 的刻画.
\end{solution}

这个例子可以很轻松地推广到一般形式，只需要将偏导数换成雅各布矩阵，这就是隐函数定理的内容. 但在正式介绍隐函数定理之前，我们看一下它线性的说法：

\begin{example}{}{}
    如果 $x = (x_1, x_2, \cdots, x_n)\in\mathbf{R}^n$ 而 $y = (y_1, y_2, \cdots, y_m)\in\mathbf{R}^m$，我们将一个点（或者说向量）$(x_1, \cdots, x_n, y_1, \cdots, y_m)\in \mathbf{R}^{n+m}$ 记作 $(x, y)$，那么每一个线性映射 $A\in \mathcal{L}(\mathbf{R}^{n+m}, \mathbf{R}^{m})$ 可以被表示成两个线性映射 $A_x$ 和 $A_y$ 的“拼接”，分别由 \[A_xh = A(h, 0),\enspace A_yk = A(0, k)\]
    确定，其中 $h\in\mathbf{R}^n$，$k\in\mathbf{R}^m$，并且 $A_x\in \mathcal{L}(\mathbf{R}^n, \mathbf{R}^m)$，$A_y\in \mathcal{L}(\mathbf{R}^m)$. 并且有 \[A(h, k) = A_xh + A_yk.\]
    如果 $A_y$ 是可逆的，那么对于每一个 $x\in\mathbf{R}^n$，存在唯一的 $y\in\mathbf{R}^m$ 使得 $A(x, y) = c$，这里的 $c\in\mathbf{R}^m$. 更进一步地，这个 $y$ 可以利用公式 \[y = -[A_y]^{-1}A_x(x) + [A_y]^{-1}c\] 确定.
\end{example}

这个例子直观且很容易证明，所以证明就此略去. 进行一些略微不严谨的思考：对于方程 $A(x, y) = c$ 来说，我们将其视为一个线性的隐函数，等式右边可以确定一个 $m$ 维的空间中的点，所以“提供了 $m$ 维的信息”，因此左侧就是一个“余 $m$ 维”式子，因此可以根据 $n$ 维的点来确定另外一个 $m$ 维的点. 现在来看隐函数定理，你会惊奇地发现隐函数定理的形式与隐函数为线性时的情况是如此相似.

\begin{theorem}{隐函数定理}{}
    设 $W\subset \mathbf{R}^{n+m}$ 为开集，$W$ 中的点用 $(x, y)$ 表示，其中 $x = (x_1, x_2, \cdots, x^n), \enspace y = (y_1, y_2, \cdots, y_m)$，$f:W\to \mathbf{R}^m$ 为 $C^k$ 映射，用分量表示为 \[f(x, y) = (f_1(x, y), f_2(x, y), \cdots, f_m(x, y)).\]
    设 $(x^0, y^0)\in W$， 且 $\det J_y(x^0, y^0)\neq 0$，其中 $J_y(x, y) = \left(\dfrac{\partial f_i}{\partial x_j}(x, y)\right)_{m\times n}$. 则存在 $x^0$ 的开邻域 $V\subset \mathbf{R}^n$ 以及唯一的 $C^k$ 映射 $\psi: V\to \mathbf{R}^m$，使得\begin{enumerate}
        \item[(1)] $\psi(x^0) = y^0,\enspace f(x, \psi(x)) = f(x^0, y^0),\enspace \forall x\in V$；
        \item[(2)] $J\psi(x) = -[J_yf(x, \psi(x))]^{-1}J_xf(x, \psi(x))$，其中 $J_xf(x, y) = \left(\dfrac{\partial f_i}{\partial x_j}(x, y)\right)_{m\times n}$.
    \end{enumerate}
\end{theorem}

隐函数定理的证明与上面的\autoref{ex:25:隐函数}几乎一模一样，可以照葫芦画瓢完成隐函数定理的证明，这里就不再赘述.

反函数定理与隐函数定理对函数在 $(x^0, y^0)$ 时的微分 $J_*f(x^0, y^0)$ （这里的 $*$ 表示隐函数对 $y$ 求微分）有着很高的要求，亦即要求它是可逆也就是\term{满秩}的，那么如果这个微分不满秩呢？下面的\term{秩定理}. 我们还是先回忆一点关于线性变换的事实.

\begin{theorem}{射影}{}
    对于一个线性空间 $X$ 上的线性变换 $P\in \mathcal{L}(X)$ 满足 $P^2 = P$，那么称 $P$ 为 $X$ 里的一个\term{射影}. 射影满足下面的性质：
    \begin{enumerate}
        \item[(1)] 每一个 $x\in X$ 都可以唯一表示成 $x = x_1+x_2$ 的形式，其中 $x_1\in\mathrm{im}A$，$x_2\in\ker A$；
        \item[(2)] 如果 $X$ 是有限维的线性空间，$X_1$ 是 $X$ 内的一个线性子空间，那么在 $X$ 中存在一个射影 $P$ 使得 $\mathrm{im}P = X_1$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item[(1)] 令 $x_1 = Px$，$x_2 = x - Px$，则 $Px_2 = Px - Px_1 = Px - P^2x = 0$，所以 $x_2\in\ker P$. 将 $P$ 作用在 $x = x_1 + x_2$ 上，有 $Px = Px_1 + Px_2 = Px_1$，所以 $x_1 = Px$，这就证明了表示的唯一性.
        \item[(2)]  如果 $X = \{0\}$，那么这是显然的. 于是我们假设 $\dim X_1 = k >0$，取 $X$ 的某个基 $\{u_1, u_2, \cdots, u_n\}$ 使得 $\{u_1, u_2, \cdots, u_k\}$ 是 $X_1$ 的基，那么我们可以定义 $P$ 使得 $P(u_1) = u_1, P(u_2) = u_2, \cdots, P(u_k) = u_k$，并且 $P(u_{k+1}) = P(u_{k+2}) = \cdots = P(u_n) = 0$，这样就构造出了一个射影.
    \end{enumerate}
\end{proof}

\begin{theorem}{秩定理}{}
    设 $m$，$n$ 和 $r$ 是非负整数，满足 $m \geqslant r$，$n\geqslant r$，$E \subset \mathbf{R}^n$ 是开集，$f \colon E \to \mathbf{R}^m$ 是连续可微映射，对于每个 $x\in E$，$Jf(x)$ 的秩为 $r$，固定 $x_0\in E$，另 $A = Jf(x_0)$，设 $A$ 的值域为 $Y_1$，$P$ 为 $\mathbf{R}^m$ 的射影，其值域也是 $Y_1$，$Y_2 = \ker P$. 则在 $\mathbf{R}^n$ 中存在着开集 $U\subset E$ 和 $V$，$x \in U$，并且存在着从 $V$ 映满到 $U$ 的连续可微的一一映射 $H$，其逆也是连续可微的，使得 \[f(H(x)) = Ax + \varphi(Ax), \enspace \forall x\in V,\]
    这里的 $\varphi$ 是一个将开集 $A(V)\subset Y_1$ 映到 $Y_2$ 的连续可微映射.
\end{theorem}

\begin{proof}
    若 $r = 0$，则 $A$ 的每个元素都为 $0$，这表明在 $x_0$ 的某个小邻域内，$f$ 是常值函数，这时令 $V = U$，$H$ 为恒等映射，$\varphi(0) = f(x_0)$ 即可. 所以我们只需考虑 $r > 0$ 的情况.

    因为 $\dim Y_1 = r$，$Y_1$ 一定存在一组基 $\{y_1, y_2, \cdots, y_r\}$，我们可以选择一组向量 $z_i\in \mathbf{R}^n$ 使得对每个 $1 \leqslant i \leqslant r$，$Az_i = y_i$，并且定义从 $Y_1$ 到 $\mathbf{R}^n$ 的线性映射 $S$，使得对于一切标量 $c_1, c_2, \cdots, c_r$，\[S(c_1y_1 + c_2y_2 + \cdots + c_ry_r) = c_1z_1 + c_2z_2 + \cdots + c_rz_r.\]
    这样就有 $ASy_i = Az_i = y_i$，所以就有 \[ASy = y, \enspace \forall y\in Y_1.\]
    定义从 $E$ 到 $\mathbf{R}^n$ 内的映射 $G$ 满足：\[G(x) = x + SP[f(x) - Ax], \enspace \forall x\in E.\]
    因为 $Jf(x_0) = A$，对 $G$ 求微分则有 $JG(x_0) = I_n$，根据反函数定理，存在开集 $U$ 与 $V$，$x_0\in U$，$G$ 为将 $U$ 映满 $V$ 的连续可微一一映射，且 $G$ 的逆 $H$ 也是连续可微映射. 此外，我们还可以将 $U$ 与 $V$ 收缩一下，使 $V$ 成为凸集，这样 $JH(x)$ 就对于每个 $x\in V$ 可逆.

    注意 $ASPA = A$，这是因为 $PA = A$（为什么），并且 $ASy = y$，所以可以得到 \[AG(x) = Pf(x), \enspace \forall x\in E.\]
    特别当 $x\in U$ 时，$AG(x) = Pf(x)$，若将 $x$ 换成 $H(x)$，则有 \[Pf(H(x)) = AG(H(x)) = Ax, \enspace \forall x\in V.\]
    定义函数 $\psi(x) = f(H(x)) - Ax, \enspace x\in V$，所以根据 $PA = A$ 可以得到 $P\psi(x) = 0$，于是 $\psi$ 是从 $V$ 到 $Y_2$ 的连续可微映射.

    因为 $V$ 是开集，所以 $A(V)$ 也一定是 $A$ 的值域 $Y_1$ 的开子集. 下面必须证明存在着 $A(V)$ 到 $Y_2$ 的连续可微映射 $\varphi$，使得 $\psi(x) = \varphi(Ax)$，这样就完成了证明.

    首先证明：对于 $x_1, x_2\in V$，$Ax_1 = Ax_2$，那么 \[\psi(x_1) = f(H(x_1)) - Ax_1 = f(H(x_2)) - Ax_2 = \psi(x_2).\]
    令 $\Phi(x) = f(H(x))$，$x\in V$，因为对于每一个 $x\in V$，$JH(x)$ 的秩为 $n$，而对每个 $x\in U$，$Jf(x)$ 的秩为 $r$，所以 \[r(J\Phi(x)) = r(Jf(H(x))\cdot JH(x)) = r, \enspace \forall x \in V.\]
    固定 $x\in V$，令 $M$ 为 $J\Phi(x)$ 的值域，那么 $M\subset \mathbf{R}^m$，并且 $\dim M = r$，所以 $PJ\Phi(x) = A$，因此 $P$ 将 $M$ 映满到 $Y_1$，因为 $\dim M = \dim Y_1$，所以 $\left.P\right|_{M}$ 是一个一一映射.

    设 $Ah = 0$，那么 $PH\Phi(x)h = 0$，但是 $J\Phi(x)h \in M$，并且 $P$ 在 $M$ 上是一一映射，因此 $J\Phi(x)h = 0$，所以到这里我们已经证明了：若 $x\in V$，且 $Ah = 0$，那么 $J\psi(x)h = 0$，往回一步，我们证明对于 $x_1, x_2\in V$，$Ax_1 = Ax_2$，就有 $\psi(x_1) = \psi(x_2)$. 令 $h = x_2 - x_1$，并定义 $g(t) = \psi(x_1 + th)$，其中 $t\in [0, 1]$，根据 $V$ 的凸性，$x_1 + th\in V$，所以 \[g'(t) = J\psi(x_1 + th)h = 0, \enspace 0 \leqslant t\leqslant 1.\]
    因此 $g(1) = g(0)$，但是 $g(0) = \psi(x_1)$，$g(1) = \psi(x_2)$，所以 $\psi(x_1) = \psi(x_2)$，这就完成了证明.

    所以对于 $x\in V$，$\psi(x)$ 只与 $Ax$ 有关，所以 $\varphi(Ax) = \psi(x)$ 确实在 $A(V)$ 上确定了 $\varphi$，这样就只需要证明 $\varphi$ 是连续可微的就可以了.

    固定 $y_0\in A(V)$，再固定 $x_0\in V$ 使得 $Ax_0 = y_0$，因为 $V$ 是开集，$y_0$ 必在 $Y_1$ 中有临域 $W$，对于任意的 $y\in W$，向量 $x = x_0 + S(y - y-0)$ 一定在 $V$ 中，所以 \[Ax = A(x_0) + y - y_0 = y.\]
    于是通过 $\varphi(Ax) = \psi(x)$，我们就得到了 \[\varphi(y) = \psi(x_0 - Sy_0 + Sy), \enspace \forall y\in W.\]
    这就说明：在 $W$ 中，$\varphi$ 是连续可微的，由于 $y_0$ 在 $A(V)$ 中是任意取的，所以在 $A(V)$ 上也自然是连续可微的. 这就完成了证明.
\end{proof}

\section{积分学}

多元函数的积分学中的核心定理主要是 Fubini 定理与重积分换元法. Fubini 定理描述了如何将一个多元函数的积分转化为一系列一元函数的积分进行计算，分析的技巧在证明中占了很大的比重，而重积分换元法表明的是如何通过坐标变换将被积函数的积分区域转化为一个更易计算的区域，其相关的技巧与思想在几何中也有很多的应用. 在这一节中，我们将仔细介绍如何通过线性函数来逼近进行区域变换的函数，使用微分学的方法来估计误差，最后得出重积分换元法.

\begin{example}{坐标变换}{}
    对于定义区域 $\Omega_1$ 和 $\Omega_2$ 如下：
    \[\Omega_1 = \mathbf{R}^2\setminus\{(x, y) \colon x\geqslant 0, y = 0\},\enspace \Omega_2 = \mathbf{R}_{>0}\times (0, 2\pi) = \{(r, \theta) \colon r>0, \theta\in(0, 2\pi)\}.\]
    我们熟悉的坐标变换 $x = r\cos\theta$，$y = r\sin\theta$ 就可以写成 \[\Phi: \Omega_2\to\Omega_1,\enspace \Phi(r, \theta) = (r\cos\theta, r\sin\theta).\]（这里要有一个图）
    由于在 $\Omega_1$ 上我们给定了 $(x, y)$ 作为坐标，在$\Omega_2$ 上我们给定了 $(r, \theta)$ 作为坐标，所以我们可以使用 Jacobi 矩阵表示上述映射的微分
    \[
        \mathrm{d}\Phi = J\Phi = \begin{pmatrix}
            \dfrac{\partial x}{\partial r} & \dfrac{\partial x}{\partial \theta} \\
            \dfrac{\partial y}{\partial r} & \dfrac{\partial y}{\partial \theta}
        \end{pmatrix} = \begin{pmatrix}
            \cos\theta & -r\sin\theta \\
            \sin\theta & r\cos\theta
        \end{pmatrix}
    \]
    根据反函数定理，这个映射当然是可逆的.
\end{example}

这个例子告诉我们：坐标变换允许我们将被积区域进行变换，比如将圆或球转化成一个矩形，这样就可以极大简化运算. 更确切来说，对于原被积区域 $\Omega$ 上的函数 $f$，我们可以定义一个映射 $\varphi: \Sigma \to \Omega,\enspace \varphi(\Sigma) = \Omega$，使得我们只需要在现被积区域 $\Sigma$ 对复合函数 $f\circ\varphi$ 进行积分，函数的复合保证了积分区域转换的合法性，但是我们并不可以草率进行 $\displaystyle\int_{\Omega}f\,\mathrm{d}x = \displaystyle\int_{\Sigma}f\circ \varphi\,\mathrm{d}x$ 的计算，因为对于 $\Omega$ 上的某一块体积元 $\sigma$，对与 $f$ 与坐标变换 $\varphi$ 下对应的 $\Sigma$ 上的体积元 $\sigma'$ 的体积并不一定相等，就好像对于上面例子中，将圆转化为矩形一样. 那么体积究竟变化了多少呢？这就是下面重积分换元法将要讨论的事情了.

下面，我们首先考虑坐标变换为线性映射的情况，再使用微分学的基本手法对一般的坐标变换做线性化并且估计误差.

(1) 平移变换. 设 $v_0$ 为一个固定的向量，考虑仿射线性变换$\varphi: \mathbf{R}^n\to \mathbf{R}^n$，$\varphi(x) = x + v_0$. 而矩形的体积显然是满足平移不变性的，对于一个可求体积的矩形 $A\subset \mathbf{R}^n$，$\varphi(A)\subset \mathbf{R}^n$当然也是可求体积的，并且体积不变.

(2) 伸缩变换. 设 $\lambda_i\in\mathbf{R}\enspace(1\leqslant i\leqslant n)$，我们考虑线性映射 $\varphi: \mathbf{R}^n\to \mathbf{R}^n$ ，
\[\varphi(x_1, x_2, \dots, x_n) = (\lambda_1x_1, \lambda_2x_2, \dots, \lambda_nx_n),\enspace (x_1, x_2, \dots, x_n)\in \mathbf{R}^.\]
矩形 $A = [a_1, b_1]\times [a_2, b_2]\times \cdots\times [a_n, b_n]$ 在 $\varphi$ 下的像仍为矩形，且体积为\[ \nu(\varphi(A)) = \vert\lambda_1\vert \vert\lambda_2\vert \cdots\vert\lambda_n\vert \nu(A) = \vert \det\varphi\vert \nu(A).\]
将矩形 $A$ 换成一般的可求体积的图形，上述公式仍然成立，这可以根据下面的覆盖引理得出：

\begin{lemma}{第一覆盖引理}{}
    设 $A$ 为 $\mathbf{R}^n$ 中的可求体积的有界集合，则任给 $\varepsilon > 0$，存在有限个矩形 $\{I_i\}$ 与 $\{J_j\}$ 使得
    \[\bigcup_i I_i\subset A\subset \bigcup_i J_j; \enspace \sum_i \nu(I_i) + \varepsilon > \nu(A) > \sum_j \nu(J_j) - \varepsilon.\]
    其中 $\{I_i\}$ 的内部互不相交.

    （这里需要一个图图）
\end{lemma}

\begin{proof}
    对于一个包含 $A$ 的一个矩形 $I$，首先可以定义一个示性函数 $\chi_A$，满足
    \[\chi(x) = \begin{cases}
        1, & x\in A,\\
        0, & x\notin A.
    \end{cases}\]
    这样 $A$ 的体积就可以表示为 $\nu(A) = \displaystyle\int_I\chi_A$. 因此对于任意的 $\varepsilon >0$，存在 $I$ 的分割 $\pi = \{I_{ij}\}$ 使得\[\lvert \sum_{ij}\chi_A(\xi_{ij})\nu(I_{ij}) - \nu(A)\rvert < \varepsilon,\enspace \forall \xi_{ij}\in I_{ij}.\]
    根据示性函数的定义，有 \[\sum_{ij}\inf_{\xi_{ij}\in I_{ij}}\chi_A(\xi_{ij})\nu(I_{ij}) = \sum_{I_{ij}\subset A}\nu(I_{ij}),\]
    所以，对于分割 $\pi$ 就有 \[\nu(A) - \varepsilon < \sum_{I_{ij}\subset A}\nu(I_{ij}) \leqslant \nu(A).\]
    同理可以有 \[\sum_{ij}\sup_{\xi_{ij}\in I_{ij}}\chi_A(\xi_{ij})\nu(I_{ij}) = \sum_{I_{ij}\cap A\neq \varnothing}\nu(I_{ij}),\]
    此时就有 \[\nu(A) < \sum_{I_{ij}\cap A\neq \varnothing}\nu(I_{ij}) < \nu(A) + \varepsilon.\]
    这就证明了第一覆盖引理.
\end{proof}

这个证明的一个副产品就是：这些内部与 $\partial A$ 有非空交集的矩形的体积之和不会超过 $2\varepsilon$，这个结论也不失为一个良好的放缩.

(3) 正交变换. 正交变换就是我们所说的实数域上的等距同构. 对于正交变换 $O\in \mathcal{L}(\mathbf{R}^n)$ 其矩阵表示为 $O_M$，那么就有 $O_MO^T_M = O_M^TO_M = I_n$. 正交变换保持向量的模长不变，那么很自然的一点就是，我们可以大胆推断正交变换保持可求体积集合的体积不变，我们下面证明这一点. 首先需要第二覆盖引理.

\begin{lemma}{第二覆盖引理}{}
    设 $A$ 是 $\mathbf{R}^n$ 中的可求体积的有界集合，则任给 $\varepsilon>0$，存在有限个 $n$ 维球体 $\{B_i\}$ 和 $\{B^j\}$，使得 \[\bigcap_iB_i\subset A \subset \bigcup_jB^j;\enspace \sum_j\nu(B^j) - \varepsilon < \nu(A) < \sum_j\nu(B_i) + \varepsilon,\]
    其中 $\{B_i\}$ 的内部互不相交.
    （这里需要一个图图）
\end{lemma}

\begin{proof}
    首先设 $\nu(A)>0$. 我们先取一个矩形 $I = \left[a, b\right]^n$ 使得 $A\subset I$. 酱 $I$ 作 $m^n$ 等分，当 $m$ 充分大的时候，完全包含于 $A$ 的小矩形 $\{I_i^1\}$ 的体积之和满足条件 \[\sum_i\nu(I_i^1) > \frac12\nu(A).\]
    矩形 $I_i^1$ 的内接球记为 $B_i^1$，记半径为 $1$ 的 $n$ 维球的体积为 $\omega_n$，根据球体的体积公式或者伸缩变换的结果，我们有 \[\sum_i\nu(B_i^1) = \frac{\omega_n}{2^n}\sum_i\nu(I_i^1) > \frac{\omega_n}{2^{n+1}}\nu(A).\]
    记 \[q = 1 - \frac{\omega_n}{2^{n+1}}\in(0, 1).\]
    所以有 \[ 0 < \nu\left(A \setminus \bigcup_iB_i^1\right) < q\nu(A).\]
    我们对 $A\setminus \bigcup_iB_i^1$ 重复上述过程，可以得到包含于 $A\setminus \bigcup_iB_i^1$ 有限个小球体 $\{B_{i'}^2\}$ 使得 \[0 < \nu\left(A\setminus \bigcup_iB_i^1\setminus \bigcup_{i'}B_{i'}^2\right) < q\nu\left(A\setminus \bigcup_iB_i^1\right) < q^2\nu(A).\]
    将这一部分不断重复下去，对于任意的 $\varepsilon > 0$，由于 $q^k\to 0\enspace (k\to \infty)$，所以当 $k$ 充分大的时候，我们就得到内部互不相交的一列 $n$ 维球体 $\{B_i\}$ 使得 \[0 < \nu\left(A\setminus \bigcup_iB_i\right) < q^k\nu(A) < n^{-n/2}\frac{\varepsilon}{2}.\]
    现在考虑对于 $\overline{A} = A\setminus \cup_iB_i$，仍然考虑矩形 $I$ 的 $m^n$ 等分，当 $m$ 充分大的时候，存在着覆盖 $\overline{A}$ 的小矩形 $\{I^j\}$ 使得 \[\sum_j\nu(I^j) < \nu(\overline{A}) + n^{-n/2}\frac{\varepsilon}{2} < n^{-n/2}\varepsilon.\]
    矩形 $I^j$ 的外接球记为 $B_2^j$，那么就有 \[\sum_j\nu(B_2^j) = \frac{\omega_nn^{n/2}}{2^n}\sum_j\nu(I^j) < \frac{\omega_nn^{n/2}}{2^n}n^{-n/2}\varepsilon \leqslant \varepsilon.\]
    这就说明了 $\{B_i, B_2^j\}$ 就是覆盖 $A$ 的 $n$ 维球体，注意到最后一段的证明对于体积为零的情形同样适用，这就完成了第二覆盖引理的证明.
\end{proof}

\begin{theorem}{}{}
    正交变换保持体积不变.
\end{theorem}

\begin{proof}
    注意到正交变换将 $n$ 维球映为 $n$ 维球，且球的半径不变，进而其体积不变. 根据第二覆盖引理，正交变换将零体积映为零体积集. 再注意到正交变换将集合的边界点映射为边界点，内点映射为内点，因此将可求体积的集合映为可求体积的集合. 再由覆盖引理及其正交变换保持球体体积变则可知正交变换保持可求体积的集合的体积不变.
\end{proof}

(4) 一般的线性变换. 设 $\varphi: \mathbf{R}^n\to \mathbf{R}^n$ 为线性映射，在 $\mathbf{R}^n$ 的标准基下可以表示为 $n$ 阶方阵，这个方阵仍然记为 $\varphi$. 对于 $\mathbf{R}^n$ 中的可求体积的有界集合 $A$，我们考虑在 $\varphi$ 下的像 $\varphi(A)$. 首先，如果 $\det\varphi = 0$，那么 $\varphi$ 就将 $A$ 压缩成了被某一个超平面包含的集合，这样的集合体积为 $0$，所以我们只考虑 $\det\varphi\neq 0$ 的情况. 这时候 $\varphi\varphi^T$ 就是正定对称矩阵，根据实谱定理，其可以对角化并且其特征值都大于零，所以其有一个正的平方根 $P$，这样 $\varphi\varphi^T$ 就可以写为 \[\varphi\varphi^T = P^2,\]
其中 $P$ 当然也是正定对称的，且 $\det P = \vert\det\varphi\vert$. 这允许我们构造一个正交矩阵 $O = P^{-1}\varphi$，读者可以自行验证它是正交的.

% 这里的 diag 没有对应的命令，但是为了看起来好一些（？，我在前面加了个 \,
停下来看一下正定对称矩阵 $P$ 的性质：其对应的线性映射是一个正且自伴的线性映射，我们将其对角化为 $P = O\,\mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_n) O^T$，其中 $\lambda_i > 0,\enspace 1\leqslant i\leqslant n$，且 $O$ 是一个正交变换. 根据正交变换与伸缩变换的结果，我们得到 \[\nu(P(A)) = (\det P)\nu(A).\]
结合上面的分析，我们将一个非退化的线性映射 $\varphi$ 分解为一个正交变换与一个正定自伴映射，这就表明如果 $A$ 是可求体积的图形，那么 $\varphi(A) = P(O(A))$ 也是可求体积的，且有 \[\nu(\varphi(A)) = \nu(P(O(A))) = (\det P)\nu(O(A)) = (\det P)\nu(A) = \vert\det\varphi\vert \nu(A).\]
简而言之，我们将上述分析总结为下面的定理：

\begin{theorem}{}{}
    对于一般的线性映射 $\varphi: \mathbf{R}^n\to \mathbf{R}^n$，与一个可求体积的图形 $A$，其像 $\varphi(A)$ 也是可求体积的，且有 \[\nu(\varphi(A)) = \vert\det\varphi\vert \nu(A).\]
\end{theorem}

现在我们要考虑比线性映射更一般的映射，根据“连续可微映射在某一点的局部性质与其在该点的微分的性质相同”这一核心原理，我们可以猜测对于某一点 $P$，如果 $\det J\varphi(P)\neq 0$，对于 $P$ 附近的一个非常小的可求体积的图形 $A$，有 $\nu(\varphi(A)) = \vert\det J\varphi(P)\vert \nu(A)$. 对这样的一个个小的面积元进行积分，就是重积分换元法的核心想法. 下面我们给出详细的证明.

设 $D\subset \mathbf{R}^n$ 为开集，$\varphi: D\to \mathbf{R}^n$ 为 $C^1$ 映射，根据拟微分中值定理，我们可以知道 $\varphi$ 是一个局部 Lipschitz 映射，也就是任取一个闭集 $P\subset D$ 与点 $x, y\in P$，存在 $\rho > 0$ 使得 \[\vert \varphi(x) - \varphi(y)\vert \leqslant \Vert J\varphi(\xi)\Vert \vert x - y\vert \leqslant \rho \vert x - y\vert.\]
所以下面讨论的主题就是如何对可求体积的集合 $A$ 在这样一个局部 Lipschitz 映射下的像 $\varphi(A)$ 的体积进行估计.

\begin{lemma}{}{}
    设 $\varphi:\mathbf{R}^n\to \mathbf{R}^n$ 为一个 Lipschitz 映射，亦即满足\[\Vert \varphi(x) - \varphi(y)\Vert \leqslant \rho \Vert x - y\Vert,\enspace \forall x, y\in\mathbf{R}^n.\]
    对于一个可求体积的集合 $A\subset \mathbf{R}^n$，如果 $A$ 为零测集，那么 $\varphi(A)$ 也是零测集；如果 $A$ 与 $\varphi(A)$ 都是可求体积的，就有 $\nu(\varphi(A)) \leqslant \rho^n\nu(A)$.
\end{lemma}

\begin{proof}
    设 $A$ 是零测集，则任给 $\varepsilon > 0$，存在至多可数个球体 $B_i = B_{r_i}(x^i)$ 使得 \[A\subset \displaystyle\bigcup_{i\geqslant 1}B_i,\enspace \sum_{i\geqslant 1}\nu(B_i) = \sum_{i\geqslant 1}\omega_n r_i^n < \varepsilon.\]
    由 $\varphi$ 是 Lipschitz 的可以得出 $\varphi(B_i) \subset B_{\rho r_i}(\varphi(x^i))$，所以 \[\sum_{i\geqslant 1}\nu(B_{\rho r_i}(\varphi(x^i))) = \sum_{i\geqslant 1}\omega_n (\rho r_i)^n = \rho^n\sum_{i\geqslant 1}\omega_n r_i^n < \rho^n\varepsilon.\]
    这就说明 $\varphi(A)$ 也是零测集.

    设 $A$ 和 $\varphi(A)$ 都是可求体积的，我们使用第二覆盖引理，对于任给的 $\varepsilon > 0$，存在有限个可以覆盖 $A$ 的 $n$ 维球体 $\{B_{r_j}^j\}$ 使得 \[\sum_j\omega_n r_j^n < \nu(A) + \varepsilon.\]
    所以此时 $\varphi(A)$ 被 $\{B_{\rho r_j}^j\}$ 覆盖，因此 \[\nu(\varphi(A))\leqslant \sum_{j}\omega_n (\rho r_j)^n = \rho^n\sum_j\omega_n r_j^n < \rho^n(\nu(A) + \varepsilon).\]
    通过 $\varepsilon$ 的任意性可以得出 $\nu(\varphi(A)) \leqslant \rho^n\nu(A)$.
\end{proof}

事实上，通过反函数定理，如果 $J\varphi$ 处处非退化，那么 $\varphi$ 将内点映射为内点，这就说明如果 $A$ 可求体积，并且 $\overline{A}\subset D$，那么我们对 $\partial \varphi(A)$ 进行放缩：\[\partial \varphi(A) = \overline{\varphi(A)}\setminus \varphi(A)^{\circ} \subset \varphi(\overline{A})\setminus \varphi(A^{\circ}) \subset \varphi(\partial A),\]
由 $A$ 可求体积可以得到 $\partial A$ 是零测集，所以 $\varphi(\partial A)$ 也是零测集，$\partial \varphi(A)$ 仍是零测集，所以 $\varphi(A)$ 就可求体积. 上面的定理只不过给出了 $\varphi(A)$ 体积的一个简单刻画，我们洗面将 $\varphi$ 进行线性化并且进行误差估计. 取 $\delta > 0$ 使得 $K = \{x\vert d(x, A) \leqslant \delta\} \subset D$，并记 $C = \max_{x\in K}\Vert J\varphi(x)\Vert$，回忆先前对线性映射与其线性化的映射之间的误差估计，我们可以直接得到下面引理：

\begin{lemma}{}{}
    沿用上面的记号，任给 $\varepsilon >0$，存在 $0<\eta <\delta$，使得当 $x\in A$，$d(x', x)\leqslant \eta$ 时，有 \[\Vert \varphi(x') - \varphi(x) - J\varphi(x)(x' - x)\Vert\leqslant \varepsilon\Vert x' - x\Vert.\]
\end{lemma}

\begin{lemma}{}{}
    沿用上面的记号，则当 $B\subset A$ 可求体积且 $d(B) < \eta$ 时，有 \[\nu(\varphi(B)) \leqslant \left[\vert \det J\varphi(x)\vert + O(\varepsilon)\right]\nu(B),\enspace x\in B.\]
\end{lemma}

\begin{proof}
    我们考虑线性变换 $L(y) = \left[J\varphi(x)\right]^{-1}(y - \varphi(x)) + x$ 并记 $F(y) = \varphi(y) - \varphi(x) - J\varphi(x)(x'-x)$，则 \[L\circ\varphi(x') = \left[J\varphi(x)\right]^{-1} F(x') + x'.\]
    于是当 $x', x''\in B_\eta(x)$ 的时候，\[L\circ\varphi(x') - L\circ\varphi(x'') = \left[J\varphi(x)\right]^{-1} (F(x') - F(x'')) + (x' - x'')\leqslant (1 + C\varepsilon)\Vert x' - x''\Vert.\]
    通过 $B\subset B_\eta(x)$ 可以得到 $\nu(L\circ\varphi(B)) \leqslant (1 + C\varepsilon)^n\nu(B)$，所以\[\nu(\varphi(B)) = \frac{\nu(L\circ\varphi(B))}{\vert \det L\vert}\leqslant \vert \det J\varphi(x)\vert \cdot (1 + C\varepsilon)^n\nu(B) = \left[\vert \det J\varphi(x)\vert + O(\varepsilon)\right].\]
    这样就得到了证明.
\end{proof}

\begin{theorem}{重积分换元法}{}
    设 $\varphi: D\to \mathbf{R}^n$ 为 $C^1$ 的一个单射，且 $J\varphi$ 处处非退化. 设 $A\subset D$ 为可求体积的集合，$\overline{A}\subset D$，$f$ 在 $\varphi(A)$ 上可积，那么\[\int_{\varphi(A)}f = \int_A f\circ \varphi\vert \det J\varphi\vert.\]
    特别地，\[\nu(\varphi(A)) = \int_A \vert \det J\varphi\vert.\]
\end{theorem}

\begin{proof}
    不妨设 $A$ 为一个矩形，且 $f$ 非负，对于 $A$ 的任意一个分割 $\pi = \{A_{ij}\}$，我们有\[\int_{\varphi(A)}f = \sum_{i, j}\int_{\varphi(A_{ij})}f \leqslant \sum_{i, j}[\sup\limits_{\varphi(A_{ij})}f]\nu(\varphi(A_{ij})).\]
    对于任意的 $\varepsilon$ 当分割充分细，使得 $d(A_{ij})$ 小于某一个 $\eta$ 时，由上述的引理我们知道\[\int_{\varphi(A)}f \leqslant \sum_{i, j}\sup\limits_{A_{i, j}}[f\circ\varphi] \vert\det J\varphi(\xi_{ij})\vert \nu(A_{ij}) + O(\varepsilon) = \int_{A}f\circ\varphi\vert\det J\varphi\vert + O(\varepsilon).\]
    另 $\varepsilon\to0^+$ 可以得到\[\int_{\varphi(A)}f\leqslant \int_A f\circ\varphi\vert\det J\varphi\vert.\]
    根据反函数定理，$\varphi:D\to \varphi(D)$ 是可逆的，所以我们对于 $\varphi^{-1}$ 与 $\displaystyle\int_{A}f\circ\varphi\vert\det J\varphi\vert$ 进行类似的论证，可以得到 \[\int_{A}f \circ \varphi\vert \det J\varphi\vert \leqslant \int_{\varphi(A)}f\circ \varphi\circ\varphi^{-1}\vert\det J\varphi\vert \cdot\vert \det J\varphi^{-1}\vert  = \int_{\varphi(A)}f.\]
    综上所述，我们有\[\int_{\varphi(A)}f = \int_{A}f\circ\varphi\vert\det J\varphi\vert.\]这就完成了证明.
\end{proof}

这样，我们其实可以发现：重积分换元法的核心想法其实是研究\term{可求体积的图形在某一个映射下的体积是如何变化的}. 以这个想法为主线，配合\term{连续可微映射在某一点的局部性质与其在该点的微分的性质相同}这一核心原理，从线性映射到误差估计，我们就完成了重积分换元法的证明.

\vspace{2ex}
\centerline{\heiti \Large 内容总结}

\vspace{2ex}
\centerline{\heiti \Large 习题}

\vspace{2ex}
{\kaishu }
\begin{flushright}
    \kaishu

\end{flushright}

\centerline{\heiti A组}
\begin{enumerate}
    \item
\end{enumerate}

\centerline{\heiti B组}
\begin{enumerate}
    \item
\end{enumerate}

\centerline{\heiti C组}
\begin{enumerate}
    \item
\end{enumerate}
